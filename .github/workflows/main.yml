name: üóìÔ∏è Daily Sitemap Build & Publish

on:
  schedule:
    - cron: "0 1 * * *"   # 01:00 UTC daily
  workflow_dispatch:

permissions:
  contents: read

jobs:
  build-publish:
    runs-on: ubuntu-latest

    env:
      # ===== customize these =====
      CSV_URL: https://storage.googleapis.com/sitemaps.leeladiamond.com/Googlefinal/combined_google_merchant_feed.csv
      LINK_COL: link
      GCS_BUCKET: sitemaps.leeladiamond.com
      OUTPUT_DIR: ./sitemaps_out
      PUBLIC_BASE_URL: https://www.leeladiamond.com/sitemaps
      PER_FILE: "50000"
      SITEMAP_INDEX_NAME: sitemap-index.xml
      SITEMAP_PREFIX: leela-products-
      # ===========================

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Cache pip
        uses: actions/cache@v4
        with:
          path: ~/.cache/pip
          key: ${{ runner.os }}-pip-v1-${{ hashFiles('**/requirements.txt') }}
          restore-keys: |
            ${{ runner.os }}-pip-v1-

      - name: Install Python deps
        run: |
          python -m pip install --upgrade pip
          pip install pandas lxml requests

      # ---------- QUICK INPUT CHECKS ----------
      - name: Check CSV is reachable and has the column
        env:
          CSV_URL: ${{ env.CSV_URL }}
          LINK_COL: ${{ env.LINK_COL }}
        run: |
          python - <<'PY'
          import os, sys, pandas as pd
          url = os.environ["CSV_URL"]
          link_col = os.environ.get("LINK_COL", "link")
          try:
              df = pd.read_csv(url, dtype=str, nrows=5)
          except Exception as e:
              print(f"ERROR: Can't read CSV: {e}")
              sys.exit(1)
          if link_col not in df.columns:
              print(f"ERROR: Column '{link_col}' not found. Columns: {list(df.columns)}")
              sys.exit(1)
          print("CSV reachable and column present.")
          PY

      # ---------- AUTH TO GCP ----------
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}

      - name: Setup gcloud SDK (gsutil)
        uses: google-github-actions/setup-gcloud@v2
        with:
          install_components: "gsutil"

      # ---------- GENERATE SITEMAPS ----------
      - name: Generate sitemaps from CSV (streaming, memory-safe)
        env:
          CSV_URL: ${{ env.CSV_URL }}
          OUTPUT_DIR: ${{ env.OUTPUT_DIR }}
          PUBLIC_BASE_URL: ${{ env.PUBLIC_BASE_URL }}
          PER_FILE: ${{ env.PER_FILE }}
          LINK_COL: ${{ env.LINK_COL }}
          SITEMAP_PREFIX: ${{ env.SITEMAP_PREFIX }}
          SITEMAP_INDEX_NAME: ${{ env.SITEMAP_INDEX_NAME }}
        run: |
          python - <<'PY'
          # -*- coding: utf-8 -*-
          import os, sys, datetime
          import pandas as pd

          CSV_URL = os.environ["CSV_URL"]
          OUTPUT_DIR = os.environ["OUTPUT_DIR"]
          PUBLIC_BASE_URL = os.environ["PUBLIC_BASE_URL"].rstrip("/")
          PER_FILE = int(os.environ.get("PER_FILE", "50000"))
          LINK_COL = os.environ.get("LINK_COL", "link")
          SITEMAP_PREFIX = os.environ.get("SITEMAP_PREFIX", "leela-products-")
          SITEMAP_INDEX_NAME = os.environ.get("SITEMAP_INDEX_NAME", "sitemap-index.xml")

          os.makedirs(OUTPUT_DIR, exist_ok=True)

          def iter_links(csv_source, link_col="link", chunksize=200_000):
            for chunk in pd.read_csv(csv_source, dtype=str, usecols=[link_col], chunksize=chunksize):
              series = chunk[link_col].dropna().astype(str).str.strip()
              for url in series[series.ne("")]:
                yield url

          def write_urlset_xml(file_path, urls):
            with open(file_path, "w", encoding="utf-8", newline="") as f:
              f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
              f.write('<urlset xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n')
              for u in urls:
                u = (u.replace("&","&amp;").replace("<","&lt;").replace(">","&gt;"))
                f.write("  <url>\n")
                f.write(f"    <loc>{u}</loc>\n")
                f.write("  </url>\n")
              f.write("</urlset>\n")

          def write_index_xml(index_path, part_files):
            now = datetime.datetime.utcnow().replace(microsecond=0).isoformat() + "Z"
            with open(index_path, "w", encoding="utf-8", newline="") as f:
              f.write('<?xml version="1.0" encoding="UTF-8"?>\n')
              f.write('<sitemapindex xmlns="http://www.sitemaps.org/schemas/sitemap/0.9">\n')
              for name in part_files:
                loc = f"{PUBLIC_BASE_URL}/{name}"
                f.write("  <sitemap>\n")
                f.write(f"    <loc>{loc}</loc>\n")
                f.write(f"    <lastmod>{now}</lastmod>\n")
                f.write("  </sitemap>\n")
              f.write("</sitemapindex>\n")

          links_iter = iter_links(CSV_URL, LINK_COL)
          buffer, part_names = [], []
          part = 1

          for url in links_iter:
            buffer.append(url)
            if len(buffer) >= PER_FILE:
              part_name = f"{SITEMAP_PREFIX}{part:05d}.xml"
              write_urlset_xml(os.path.join(OUTPUT_DIR, part_name), buffer)
              part_names.append(part_name)
              buffer.clear()
              part += 1

          if buffer:
            part_name = f"{SITEMAP_PREFIX}{part:05d}.xml"
            write_urlset_xml(os.path.join(OUTPUT_DIR, part_name), buffer)
            part_names.append(part_name)

          if not part_names:
            print("ERROR: No URLs found. Aborting.")
            sys.exit(1)

          index_path = os.path.join(OUTPUT_DIR, SITEMAP_INDEX_NAME)
          write_index_xml(index_path, part_names)

          print(f"Generated {len(part_names)} sitemap part files; index at {index_path}")
          PY

      - name: Verify output exists
        env:
          OUTPUT_DIR: ${{ env.OUTPUT_DIR }}
          SITEMAP_INDEX_NAME: ${{ env.SITEMAP_INDEX_NAME }}
        run: |
          test -d "${OUTPUT_DIR}" || (echo "Missing ${OUTPUT_DIR}" && exit 1)
          ls -la "${OUTPUT_DIR}"
          test -f "${OUTPUT_DIR}/${SITEMAP_INDEX_NAME}" || (echo "${SITEMAP_INDEX_NAME} not found" && exit 1)

      - name: Upload sitemap artifacts (for debugging)
        uses: actions/upload-artifact@v4
        with:
          name: generated-sitemaps
          path: ${{ env.OUTPUT_DIR }}
          if-no-files-found: error
          retention-days: 5

      # ---------- UPLOAD TO GCS ----------
      - name: Upload sitemaps to GCS (/sitemaps/)
        env:
          GCS_BUCKET: ${{ env.GCS_BUCKET }}
          OUTPUT_DIR: ${{ env.OUTPUT_DIR }}
        run: |
          gsutil -m rsync -r -d "${OUTPUT_DIR}" "gs://${GCS_BUCKET}/sitemaps"

      - name: Set Cache-Control & Content-Type
        env:
          GCS_BUCKET: ${{ env.GCS_BUCKET }}
        run: |
          gsutil -m setmeta \
            -h "Cache-Control:public, max-age=3600" \
            -h "Content-Type:application/xml; charset=utf-8" \
            "gs://${GCS_BUCKET}/sitemaps/*.xml" || true

      # ---------- PURGE CLOUDFLARE CACHE (token & zone optional) ----------
      - name: Purge Cloudflare cache for sitemap index (optional)
        env:
          CF_ZONE_ID: ${{ secrets.CF_ZONE_ID }}
          CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
          SITEMAP_INDEX_URL: ${{ env.PUBLIC_BASE_URL }}/${{ env.SITEMAP_INDEX_NAME }}
        run: |
          if [ -n "${CF_ZONE_ID:-}" ] && [ -n "${CF_API_TOKEN:-}" ]; then
            echo "Purging Cloudflare cache for ${SITEMAP_INDEX_URL}"
            curl -sS -X POST "https://api.cloudflare.com/client/v4/zones/${CF_ZONE_ID}/purge_cache" \
              -H "Authorization: Bearer ${CF_API_TOKEN}" \
              -H "Content-Type: application/json" \
              --data "{\"files\":[\"${SITEMAP_INDEX_URL}\"]}" | tee /tmp/cf_purge.json
            echo "Cloudflare purge response:" && cat /tmp/cf_purge.json
          else
            echo "CF_ZONE_ID or CF_API_TOKEN not set ‚Äî skipping Cloudflare purge."
          fi
